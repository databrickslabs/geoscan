/*
 * Copyright 2021 Databricks, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.databricks.labs.gis.ml

import org.apache.hadoop.fs.Path
import org.apache.spark.SparkContext
import org.apache.spark.ml.param.{ParamPair, Params}
import org.json4s.DefaultFormats
import org.json4s.JsonAST.{JObject, JValue}
import org.json4s.JsonDSL._
import org.json4s.jackson.JsonMethods.{compact, parse, render}

object ModelIOUtils {

  /**
   * Although Spark should know how to save / load metadata using DefaultParamsReader, it (for some blurry reason) does not
   * understand how to save / load both parameter and additional data. So we have to handle our parameters (de)serialization manually
   * @param uid the model UUID as generated by Spark framework
   * @param params the model parameters to save
   */
  case class Metadata(uid: String, params: JValue) {
    def getParamValue(paramName: String): JValue = {
      implicit val format: DefaultFormats.type = DefaultFormats
      params match {
        case JObject(pairs) =>
          val values = pairs.filter { case (pName, _) =>
            pName == paramName
          }.map(_._2)
          assert(values.length == 1, s"Expected one instance of Param '$paramName' but found ${values.length}")
          values.head
        case _ => throw new IllegalArgumentException(s"Cannot recognize JSON metadata")
      }
    }
  }

  /**
   * Given our model, we store metadata to disk as a simple JSON file
   * @param instance the model to save
   * @param path the path where to store our model (distributed file system)
   * @param sc the spark context, implicitly provided by Spark API
   */
  def saveMetadata(instance: Params, path: String, sc: SparkContext): Unit = {
    val metadataPath = new Path(path, "metadata").toString
    val metadataJson = getMetadataToSave(instance)
    sc.parallelize(Seq(metadataJson), 1).saveAsTextFile(metadataPath)
  }

  /**
   * As our model extends Param interface, we can easily extract all parameters as JSON elements
   * @param instance the model to save
   * @return a JSON representation of all our parameters as defined in our pipeline
   */
  def getMetadataToSave(instance: Params): String = {
    val uid = instance.uid
    val params = instance.extractParamMap().toSeq.asInstanceOf[Seq[ParamPair[Any]]]
    val jsonParams = render(params.map { case ParamPair(p, v) => p.name -> parse(p.jsonEncode(v)) }.toList)
    val metadata = ("uid" -> uid) ~ ("paramMap" -> jsonParams)
    compact(render(metadata))
  }

  /**
   * As we stored metadata as a single JSON record, we read as text and map each value to their respective parameter to set
   * @param path where to read data from
   * @param sc the spark context, implicitly provided by Spark API
   * @return a [[Metadata]] case class containing all of our parameters
   */
  def loadMetadata(path: String, sc: SparkContext): Metadata = {
    val metadataPath = new Path(path, "metadata").toString
    val metadataStr = sc.textFile(metadataPath, 1).first()
    implicit val format: DefaultFormats.type = DefaultFormats
    val metadata = parse(metadataStr)
    val uid = (metadata \ "uid").extract[String]
    val params = metadata \ "paramMap"
    Metadata(uid, params)
  }

  /**
   * Given the metadata extracted (included its UUID), we configure our model instance with all the necessary parameters
   * @param instance the model created
   * @param metadata the metadata extracted
   */
  def getAndSetParams(instance: Params, metadata: Metadata): Unit = {
    implicit val format: DefaultFormats.type = DefaultFormats
    metadata.params match {
      case JObject(pairs) =>
        pairs.foreach { case (paramName, jsonValue) =>
          val param = instance.getParam(paramName)
          val value = param.jsonDecode(compact(render(jsonValue)))
          instance.set(param, value)
        }
      case _ => throw new IllegalArgumentException(s"Cannot recognize JSON metadata")
    }
  }

}
